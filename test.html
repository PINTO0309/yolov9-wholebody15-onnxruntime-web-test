<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ONNX 推論とバウンディングボックスの描画</title>
  <style>
    canvas {
      border: 1px solid black;
    }
  </style>
</head>
<body>

  <!-- ローカルファイルを選択するためのファイル入力 -->
  <input type="file" id="file-input" accept="image/png">

  <!-- 選択した画像を表示するための img 要素 -->
  <img id="selected-image" src="" alt="Selected Image" style="display:none; max-width: 100%; height: auto;">

  <!-- 推論結果を描画するための canvas -->
  <canvas id="output-canvas"></canvas>

  <!-- 推論を実行するボタン -->
  <button id="run-inference">Run Inference</button>

  <!-- 推論時間を表示するための要素 -->
  <div id="inference-time" style="margin-top: 10px;"></div>

  <!-- onnxruntime-web を CDN から読み込む -->
  <!-- https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.all.js"></script>

  <script>
    const MODEL_INPUT_WIDTH = 640;   // モデルの入力幅
    const MODEL_INPUT_HEIGHT = 480;  // モデルの入力高

    // 15クラス用の視認性の良い配色
    const classColors = [
      '#FF6347',  // トマト色（赤系）
      '#4682B4',  // スチールブルー（青系）
      '#32CD32',  // ライムグリーン（緑系）
      '#FFD700',  // ゴールド（黄色系）
      '#8A2BE2',  // ブルーバイオレット（紫系）
      '#FF4500',  // オレンジレッド（赤系）
      '#1E90FF',  // ドッジャーブルー（青系）
      '#3CB371',  // ミディアムシーグリーン（緑系）
      '#DAA520',  // ゴールデンロッド（黄色系）
      '#9400D3',  // ダークバイオレット（紫系）
      '#FF1493',  // ディープピンク（ピンク系）
      '#00CED1',  // ダークターコイズ（水色系）
      '#ADFF2F',  // グリーンイエロー（黄緑系）
      '#FFDAB9',  // ピーチパフ（薄いオレンジ系）
      '#B22222'   // ファイアブリック（濃い赤系）
    ];

    // 画像をロードして表示する関数
    function loadImage(file) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = (e) => {
          const imgElement = document.getElementById('selected-image');
          imgElement.src = e.target.result;
          imgElement.style.display = 'block'; // 画像を表示する
          imgElement.onload = () => resolve(imgElement); // 画像のロード完了後に解決
        };
        reader.onerror = reject;
        reader.readAsDataURL(file); // 画像をData URL形式で読み込む
      });
    }

    // 画像を元のサイズで BGR に変換する関数
    function preprocessImage(imageElement) {
      const canvas = document.createElement('canvas');
      const width = imageElement.width;
      const height = imageElement.height;
      canvas.width = width;
      canvas.height = height;

      const ctx = canvas.getContext('2d');
      ctx.drawImage(imageElement, 0, 0, width, height);

      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
      const { data } = imageData;

      const inputTensor = new Float32Array(1 * 3 * height * width);
      let offset = 0;

      for (let i = 0; i < data.length; i += 4) {
        // RGB -> BGR 変換
        inputTensor[offset] = data[i + 2];       // B
        inputTensor[offset + width * height] = data[i + 1]; // G
        inputTensor[offset + 2 * width * height] = data[i]; // R
        offset++;
      }

      return new ort.Tensor('float32', inputTensor, [1, 3, height, width]);
    }

    // ONNX モデルのロード
    async function loadModel() {
      try {
        const session = await ort.InferenceSession.create('./yolov9_n_wholebody15_post_0145_1x3x480x640.with_runtime_opt.ort', {executionProviders: ["wasm"]});
        return session;
      } catch (err) {
        console.error('モデルのロードに失敗しました:', err);
        alert('モデルのロードに失敗しました。');
      }
    }

    // 推論の実行と時間計測
    async function runInference(session, inputTensor) {
      try {
        const startTime = Date.now();  // 推論開始時間
        const feeds = { 'input_bgr': inputTensor };  // 入力OPの名前を 'input_bgr' に変更
        const output = await session.run(feeds);
        const endTime = Date.now();  // 推論終了時間
        const inferenceTime = endTime - startTime;  // 推論時間を計算
        document.getElementById('inference-time').textContent = `推論時間: ${inferenceTime} ミリ秒`;
        console.log('推論結果:', output);  // 推論結果を確認
        return output;
      } catch (err) {
        console.error('推論に失敗しました:', err);
        alert('推論に失敗しました。');
      }
    }

    // 推論結果に基づいてバウンディングボックスを描画
    function renderBoundingBoxes(output, imageElement) {
      const canvas = document.getElementById('output-canvas');
      const width = imageElement.width;
      const height = imageElement.height;
      canvas.width = width;
      canvas.height = height;

      const ctx = canvas.getContext('2d');
      ctx.clearRect(0, 0, canvas.width, canvas.height); // 以前の内容をクリア
      ctx.drawImage(imageElement, 0, 0, canvas.width, canvas.height);

      const detections = output['batchno_classid_score_x1y1x2y2']?.data;  // 出力OPの名前に対応

      if (!detections || detections.length === 0) {
        alert('推論結果が無効です。');
        return;
      }

      const threshold = 0.5; // スコアのしきい値

      // 出力が [N, 7] の形式であることを確認して処理
      for (let i = 0; i < detections.length; i += 7) {
        const batchno = detections[i];
        const classid = detections[i + 1];
        const score = detections[i + 2];
        let x1 = detections[i + 3];
        let y1 = detections[i + 4];
        let x2 = detections[i + 5];
        let y2 = detections[i + 6];

        if (score > threshold) {
          // キャンバスのスケールに合わせて座標を変換
          const scaleX = canvas.width / MODEL_INPUT_WIDTH;
          const scaleY = canvas.height / MODEL_INPUT_HEIGHT;

          x1 = x1 * scaleX;
          y1 = y1 * scaleY;
          x2 = x2 * scaleX;
          y2 = y2 * scaleY;

          // クラスIDに対応する色を選択
          const boxColor = classColors[classid % classColors.length];

          // バウンディングボックスの描画
          ctx.strokeStyle = boxColor;
          ctx.lineWidth = 2;
          ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);

          // 信頼度スコアの表示
          ctx.font = '16px Arial';
          ctx.fillStyle = boxColor;
          //ctx.fillText(`Score: ${score.toFixed(2)}`, x1, y1 - 10); // ボックスの上にスコア表示

          // クラスIDの表示
          //ctx.fillText(`Class: ${classid}`, x1, y1 - 25);  // スコアの上にクラスID表示
        }
      }
    }

    // ファイルが選択されたときに発生するイベント
    document.getElementById('file-input').addEventListener('change', async (event) => {
      const file = event.target.files[0];
      if (file) {
        const imageElement = await loadImage(file); // 画像をロードして表示
        const canvas = document.getElementById('output-canvas');
        canvas.width = imageElement.width;
        canvas.height = imageElement.height;
      }
    });

    // 推論を実行するボタンに対するイベント
    document.getElementById('run-inference').addEventListener('click', async () => {
      const imageElement = document.getElementById('selected-image');
      if (imageElement.src) {
        const inputTensor = preprocessImage(imageElement);
        const session = await loadModel();
        const output = await runInference(session, inputTensor);

        // 推論結果に基づいてバウンディングボックスを描画
        renderBoundingBoxes(output, imageElement);
      }
    });
  </script>
</body>
</html>
